# Phony - Voice AI Agent

Phony is a voice AI agent built in Python that uses **Twilio ConversationRelay** and the **OpenAI Realtime API** to automate outbound phone calls and hold natural conversations. It connects to Twilio over WebSockets, streams audio and transcripts to GPT‑4o, and responds in real time.

## Prerequisites

- Python 3.9+
- Environment variables:
  - `TWILIO_ACCOUNT_SID`
  - `TWILIO_AUTH_TOKEN`
  - `TWILIO_PHONE_NUMBER`
  - `OPENAI_API_KEY` (Realtime API access required)
  - `HOST` - public URL where Twilio can reach your app (e.g. `abcd.ngrok.io`)
  - `OPENAI_VOICE` (optional) - OpenAI voice for generated speech (e.g. `alloy`, `aria`)
- A tunneling tool such as **ngrok** for exposing your local server when testing

## Setup Instructions

1. **Clone the repository**

   ```bash
   git clone <repo-url>
   cd phony
   ```

2. **Run the setup script**

   This script lives in `scripts/` and creates a virtual environment in `.venv`,
   installs dependencies, and copies `.env.example` to `.env` if it does not
   already exist. Afterwards edit `.env` with your credentials.

   ```bash
   ./scripts/setup.sh
   ```

3. **Expose your local server**

   Use ngrok (or similar) to expose port `8000` so Twilio can reach your server.

   ```bash
   ngrok http 8000
   ```

   Update your Twilio webhook URLs to point to the ngrok HTTPS address.

4. **Run the application**

   The main FastAPI app lives in `backend/main.py`. Start it with:

   ```bash
   uvicorn backend.main:app --reload
   ```

5. **Initiate an outbound call**

   A helper script `scripts/make_call.py` is provided to trigger a call using the Twilio API.

   ```bash
  python scripts/make_call.py +15551234567
  ```

### Selecting a Voice

Specify the voice for synthesized speech by setting `OPENAI_VOICE` in your `.env` file. Choose any voice supported by OpenAI, such as `alloy`, `aria`, or `verse`:

```bash
OPENAI_VOICE=aria
```

Restart the backend after changing this setting.

## Outbound Call

Configure your `.env` with your Twilio credentials, OpenAI key and the public
host of your application:

```bash
TWILIO_ACCOUNT_SID=ACxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
TWILIO_AUTH_TOKEN=your_auth_token
TWILIO_PHONE_NUMBER=+15550001111
OPENAI_API_KEY=sk-...
HOST=abcd.ngrok.io
```

Run the helper script with the destination number:

```bash
python scripts/make_call.py +15551234567
```

When the call is answered Twilio will request `https://$HOST/start_call`. If you
initiate calls directly from the Twilio Console, set the Voice webhook URL to
the same endpoint so your application returns the ConversationRelay TwiML.

## Inbound Call

Configure your Twilio phone number's **Voice webhook** to point to
`https://$HOST/receive_call`. Incoming calls will be connected to the same
ConversationRelay flow and proxied to the LLM.

Set the environment variable `REQUIRE_SUPERVISOR_FEEDBACK=true` to pause before
sending the assistant's reply to the caller. The draft response will appear on
the dashboard where a supervisor can edit or approve it before it is spoken.

## Project Structure

- `backend/` – Python backend package
  - `main.py` – FastAPI entry point
  - `relay_ws.py` – WebSocket handler for Twilio ConversationRelay
  - `openai_ws.py` – Bridge to the OpenAI Realtime API
- `scripts/` – Utility scripts like `make_call.py`
- `dashboard/` – Web dashboard for real‑time oversight

## WebSocket Relay

Twilio's ConversationRelay connects to `wss://<YOUR_HOST>/relay/ws` when a call
is established. The handler in `backend/relay_ws.py` opens a realtime WebSocket session
with OpenAI and relays incoming transcripts to GPT‑4o. Audio tokens generated by
the model are streamed back to Twilio so the caller hears the AI response with
minimal latency. Incoming events from Twilio include transcription text,
`interruptible` and `preemptible` flags, and call identifiers. Responses sent
back provide audio chunks, optional text, an `interruptible` flag, and a `last`
indicator marking the end of a turn.


## OpenAI Realtime Proxy

The `backend/openai_ws.py` module acts as a bridge between Twilio and OpenAI. It
opens a WebSocket session to `gpt-4o-realtime-preview`, forwards caller
transcripts to the model and streams token responses back to the call.
The implementation mirrors the ConversationRelay + OpenAI integration
pattern demonstrated in Twilio's official blog post and sample repo.

## Interactive Commands

The assistant can embed special tokens in its responses to control the call:

- `[[press:digits]]` &ndash; send the specified DTMF digits.
- `[[transfer:number]]` &ndash; end the ConversationRelay session and dial the given number.
- `[[end_call]]` &ndash; immediately hang up.

Include these commands exactly in the LLM's output. For example:

```
You selected account balance [[press:1]]
```

Any detected command will be executed server-side and the spoken output for that turn will be suppressed.
## Real-Time Event Streaming to Dashboard

The endpoint `/events/ws` streams structured JSON events for an active call. Connect with the query parameter `callSid` to receive updates for that session only. Events include transcripts from the caller, assistant replies and commands that were executed.

Example transcript event:
```json
{"type": "transcript", "callSid": "CA123", "speaker": "caller", "text": "hello", "timestamp": "2024-01-01T00:00:00Z"}
```

Example command event:
```json
{"type": "command_executed", "callSid": "CA123", "command": "press", "value": "1", "timestamp": "2024-01-01T00:00:01Z"}
```

## Manual Override via Dashboard

The dashboard provides simple controls for a supervisor to intervene during a
live call. Overrides are sent to the backend which then acts on the active
Twilio session.

Available routes and payloads:

- `POST /override/text` – `{ "callSid": "CA123", "text": "hello" }`
  Sends the text to the caller immediately using the OpenAI session.
- `POST /override/dtmf` – `{ "callSid": "CA123", "digit": "1" }`
  Injects a single DTMF digit into the call.
- `POST /override/end` – `{ "callSid": "CA123" }`
  Terminates the call.
- `POST /override/transfer` – `{ "callSid": "CA123", "target": "+15551234567" }`
  Transfers the caller to the given phone number.

Each action is published on the event stream so connected dashboards update in
real time.

## Clarification Escalation Flow

The assistant can request additional information from a supervising user during a
call using the special `[[request_user:...]]` command. When the backend detects
this token, it:

1. Tells the caller "Please hold while I check that." and pauses the
   conversation.
2. Emits a `query` event on the dashboard WebSocket containing the prompt.
3. Waits for the supervisor to answer via `POST /override/clarification` with
   `{ "callSid": "CA...", "response": "the account number" }`.
4. Sends the supervisor's text to the model as `supervisor: ...` and resumes the
   call with the assistant's next reply.

While awaiting input, caller transcripts are still logged but not forwarded to
the LLM. Only one outstanding query can exist at a time.

## LLM-to-LLM Demo

A standalone script is provided to showcase two GPT agents conversing with
each other while still leveraging the existing dashboard for supervision.
Run the demo and open two dashboard tabs, one for each printed ``callSid`` to
observe and intervene in real time:

```bash
python scripts/llm_duet_demo.py
```

Each agent is assigned a synthetic ``callSid`` and publishes the same event
stream as a real phone call, so overrides and other controls work unchanged.

## Logging & Monitoring

All calls emit structured JSON logs to both the console and a `call.log` file.
Entries include transcripts, assistant replies, executed commands and any
manual overrides. Latency metrics measure the delay from transcription receipt
to the GPT request, the time until the first token is streamed back, and when
playback begins.

Example log line:

```json
{"event": "transcript", "callSid": "CA123", "speaker": "caller", "text": "hello", "timestamp": "2024-01-01T00:00:00Z"}
```

Latency metrics are emitted as separate `latency` events:

```json
{"event": "latency", "callSid": "CA123", "metric": "gpt_response_ms", "ms": 520.5, "timestamp": "2024-01-01T00:00:00Z"}
```

Enable debug output by setting the `PHONY_DEBUG` environment variable before
starting the application.


## Docker Deployment

Docker files are provided to run the backend API and dashboard with Docker Compose.

### Build the images

```bash
docker-compose build
```

### Run the stack

```bash
docker-compose up
```

The backend will be available on `http://localhost:8000` and the dashboard UI on
`http://localhost:3000`. Configure your environment variables in a `.env` file in
this directory which will be loaded into the containers. When testing locally you
may expose the backend to Twilio using a tunnel such as `ngrok http 8000`.

In production deploy the containers behind HTTPS and update your Twilio webhooks
to point at the public address.

See the `docs/` directory for additional notes on project structure, usage and
LLM command syntax.
